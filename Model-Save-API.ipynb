{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7ae2a77",
      "metadata": {
        "id": "d7ae2a77"
      },
      "source": [
        "# **Modelado: Volatilidad**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "11cf53c0",
      "metadata": {
        "id": "11cf53c0",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e3a535a4",
      "metadata": {
        "id": "e3a535a4",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "btc = pd.read_csv(r'https://raw.githubusercontent.com/TawnyVTC/Proyectos_UN/refs/heads/main/2025/Deep_Learning/Data/btc_1d_with_volatility_and_lags.csv')\n",
        "btc['Open time'] = pd.to_datetime(btc['Open time'], format='%Y-%m-%d')\n",
        "\n",
        "# -----------------------\n",
        "# 1. Crear lags de volatilidad\n",
        "# -----------------------\n",
        "for lag in [7, 14, 21, 28]:\n",
        "    btc[f'Volatility_lag_{lag}'] = btc['Volatility'].shift(lag)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Crear targets futuros (predicciones de volatilidad)\n",
        "# -----------------------\n",
        "for i in range(1, 8):  # t+1 a t+7\n",
        "    btc[f'target_vol_t+{i}'] = btc['Volatility'].shift(-i)\n",
        "\n",
        "# -----------------------\n",
        "# 3. Definir features y targets\n",
        "# -----------------------\n",
        "features = [\n",
        "    'Close', 'LogReturn', 'Volatility',\n",
        "    'Volatility_lag_7', 'Volatility_lag_14', 'Volatility_lag_21', 'Volatility_lag_28'\n",
        "]\n",
        "\n",
        "targets = [f'target_vol_t+{i}' for i in range(1, 8)]\n",
        "\n",
        "# -----------------------\n",
        "# 4. Eliminar filas con NaN (por los shifts)\n",
        "# -----------------------\n",
        "btc = btc.dropna(subset=features + targets).reset_index(drop=True)\n",
        "\n",
        "# -----------------------\n",
        "# 5. Definir matrices de entrada y salida\n",
        "# -----------------------\n",
        "X = btc[features].values\n",
        "y = btc[targets].values\n",
        "dates = btc[\"Open time\"].values\n",
        "\n",
        "# -----------------------\n",
        "# 6. Construcción del objeto timeSeries\n",
        "# -----------------------\n",
        "timeSeries = np.concatenate([X, y], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "19dff0af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tsxv.splitTrainValTest import split_train_val_test_groupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def _ensure_2d(arr):\n",
        "    \"\"\"\n",
        "    Convierte el array a formato 2D.\n",
        "    Si llega 1D → (n,1); si llega 3D → (n, timesteps*feats)\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1:\n",
        "        return arr.reshape(-1, 1)\n",
        "    if arr.ndim == 2:\n",
        "        return arr\n",
        "    if arr.ndim == 3:\n",
        "        n, a, b = arr.shape\n",
        "        return arr.reshape(n, a * b)\n",
        "    raise ValueError(f\"Array con ndim={arr.ndim} no soportado por esta función.\")\n",
        "\n",
        "\n",
        "def split_and_scale(timeSeries, n_steps_input=7, n_steps_forecast=7, n_steps_jump=1, target_col=2):\n",
        "    X_list, y_list, Xcv_list, ycv_list, Xtest_list, ytest_list = split_train_val_test_groupKFold(\n",
        "        timeSeries,\n",
        "        n_steps_input,\n",
        "        n_steps_forecast,\n",
        "        n_steps_jump\n",
        "    )\n",
        "\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled = [], [], []\n",
        "    y_train_scaled, y_val_scaled, y_test_scaled = [], [], []\n",
        "    scalers_x, scalers_y = [], []\n",
        "\n",
        "    for fold in range(len(X_list)):\n",
        "        X_train_raw = _ensure_2d(X_list[fold])\n",
        "        X_val_raw   = _ensure_2d(Xcv_list[fold])\n",
        "        X_test_raw  = _ensure_2d(Xtest_list[fold])\n",
        "\n",
        "        # 👇 Aquí filtramos solo la variable objetivo (columna target)\n",
        "        y_train_raw = _ensure_2d(y_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "        y_val_raw   = _ensure_2d(ycv_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "        y_test_raw  = _ensure_2d(ytest_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "\n",
        "        # Escaladores\n",
        "        scaler_x = StandardScaler().fit(X_train_raw)\n",
        "        scaler_y = StandardScaler().fit(y_train_raw)\n",
        "\n",
        "        X_train_scaled.append(scaler_x.transform(X_train_raw))\n",
        "        X_val_scaled.append(scaler_x.transform(X_val_raw))\n",
        "        X_test_scaled.append(scaler_x.transform(X_test_raw))\n",
        "        y_train_scaled.append(scaler_y.transform(y_train_raw))\n",
        "        y_val_scaled.append(scaler_y.transform(y_val_raw))\n",
        "        y_test_scaled.append(scaler_y.transform(y_test_raw))\n",
        "\n",
        "        scalers_x.append(scaler_x)\n",
        "        scalers_y.append(scaler_y)\n",
        "\n",
        "        print(f\"Fold {fold+1}: train {X_train_raw.shape} -> val {X_val_raw.shape} -> test {X_test_raw.shape}\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_scaled,\n",
        "        'X_val': X_val_scaled,\n",
        "        'X_test': X_test_scaled,\n",
        "        'y_train': y_train_scaled,\n",
        "        'y_val': y_val_scaled,\n",
        "        'y_test': y_test_scaled,\n",
        "        'scalers_x': scalers_x,\n",
        "        'scalers_y': scalers_y\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b59078",
      "metadata": {},
      "source": [
        "from tsxv.splitTrainValTest import split_train_val_test_groupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def _ensure_2d(arr):\n",
        "    \"\"\"\n",
        "    Convierte el array a formato 2D.\n",
        "    Si llega 1D → (n,1); si llega 3D → (n, timesteps*feats)\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1:\n",
        "        return arr.reshape(-1, 1)\n",
        "    if arr.ndim == 2:\n",
        "        return arr\n",
        "    if arr.ndim == 3:\n",
        "        n, a, b = arr.shape\n",
        "        return arr.reshape(n, a * b)\n",
        "    raise ValueError(f\"Array con ndim={arr.ndim} no soportado por esta función.\")\n",
        "\n",
        "\n",
        "def preparar_volatilidad(btc, n_lag, n_outputs=7):\n",
        "    \"\"\"\n",
        "    Construye la matriz de características y targets usando volatilidad como variable objetivo.\n",
        "    \"\"\"\n",
        "    # Variables base\n",
        "    features = ['Volatility', 'Close', 'LogReturn',\n",
        "                'Volatility_lag_7', 'Volatility_lag_14', 'Volatility_lag_21', 'Volatility_lag_28']\n",
        "    targets = [f'target_t+{i}' for i in range(1, n_outputs+1)]\n",
        "\n",
        "    X = btc[features].values\n",
        "    y = btc[targets].values\n",
        "    timeSeries = np.concatenate([X, y], axis=1)\n",
        "\n",
        "    X_list, y_list, Xcv_list, ycv_list, Xtest_list, ytest_list = split_train_val_test_groupKFold(\n",
        "        timeSeries, n_lag, n_outputs, 1\n",
        "    )\n",
        "\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled = [], [], []\n",
        "    y_train_scaled, y_val_scaled, y_test_scaled = [], [], []\n",
        "    scalers_x, scalers_y = [], []\n",
        "\n",
        "    for fold in range(len(X_list)):\n",
        "        X_train_raw = _ensure_2d(X_list[fold])\n",
        "        X_val_raw = _ensure_2d(Xcv_list[fold])\n",
        "        X_test_raw = _ensure_2d(Xtest_list[fold])\n",
        "\n",
        "        y_train_raw = _ensure_2d(y_list[fold])\n",
        "        y_val_raw = _ensure_2d(ycv_list[fold])\n",
        "        y_test_raw = _ensure_2d(ytest_list[fold])\n",
        "\n",
        "        scaler_x = StandardScaler().fit(X_train_raw)\n",
        "        scaler_y = StandardScaler().fit(y_train_raw)\n",
        "\n",
        "        X_train_scaled.append(scaler_x.transform(X_train_raw))\n",
        "        X_val_scaled.append(scaler_x.transform(X_val_raw))\n",
        "        X_test_scaled.append(scaler_x.transform(X_test_raw))\n",
        "        y_train_scaled.append(scaler_y.transform(y_train_raw))\n",
        "        y_val_scaled.append(scaler_y.transform(y_val_raw))\n",
        "        y_test_scaled.append(scaler_y.transform(y_test_raw))\n",
        "\n",
        "        scalers_x.append(scaler_x)\n",
        "        scalers_y.append(scaler_y)\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "        'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test_scaled,\n",
        "        'scalers_x': scalers_x, 'scalers_y': scalers_y\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "640ccfb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_independencia_residuos(residuos, lags=10):\n",
        "    resultado = acorr_ljungbox(residuos, lags=[lags], return_df=True)\n",
        "    return resultado['lb_pvalue'].iloc[0]\n",
        "\n",
        "def _calc_metrics_per_horizon(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true, y_pred: arrays (n_samples, n_horizons)\n",
        "    Devuelve DataFrame con métricas por horizonte.\n",
        "    \"\"\"\n",
        "    n_outputs = y_true.shape[1]\n",
        "    rows = []\n",
        "    for h in range(n_outputs):\n",
        "        yt = y_true[:, h]\n",
        "        yp = y_pred[:, h]\n",
        "        mae = mean_absolute_error(yt, yp)\n",
        "        mse = mean_squared_error(yt, yp)\n",
        "        rmse = np.sqrt(mse)\n",
        "        # MAPE con epsilon para evitar división por cero\n",
        "        mape = np.mean(np.abs((yt - yp) / (np.abs(yt) + 1e-8))) * 100\n",
        "        rows.append({'Horizonte': h+1, 'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape})\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4b3e7ab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def entrenar_y_guardar_modelo(data_dict, lag, n_outputs=7, epochs=100, batch_size=32, save_dir='models/volatilidad'):\n",
        "    resultados = []\n",
        "    for fold in range(len(data_dict['X_train'])):\n",
        "        X_train, X_val, X_test = data_dict['X_train'][fold], data_dict['X_val'][fold], data_dict['X_test'][fold]\n",
        "        y_train, y_val, y_test = data_dict['y_train'][fold], data_dict['y_val'][fold], data_dict['y_test'][fold]\n",
        "        scaler_y = data_dict['scalers_y'][fold]\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(n_outputs)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                  epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[es])\n",
        "\n",
        "        yhat_test = scaler_y.inverse_transform(model.predict(X_test))\n",
        "        ytest_real = scaler_y.inverse_transform(y_test)\n",
        "\n",
        "        df_metrics = _calc_metrics_per_horizon(ytest_real, yhat_test)\n",
        "        rmse_prom = df_metrics['RMSE'].mean()\n",
        "        resultados.append((fold, rmse_prom, model))\n",
        "\n",
        "    # Guardar el mejor modelo\n",
        "    best_fold, best_rmse, best_model = min(resultados, key=lambda x: x[1])\n",
        "    model_dir = os.path.join(save_dir, f'lag_{lag}')\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, f'mejor_fold_lag_{lag}.keras')\n",
        "    best_model.save(model_path)\n",
        "    print(f\"✅ Modelo lag {lag} guardado (fold {best_fold+1}) con RMSE {best_rmse:.4f}\")\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "81d5ed4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Entrenando modelo para lag 7 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "✅ Modelo lag 7 guardado (fold 4) con RMSE 0.0566\n",
            "\n",
            "=== Entrenando modelo para lag 14 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "✅ Modelo lag 14 guardado (fold 2) con RMSE 0.0639\n",
            "\n",
            "=== Entrenando modelo para lag 21 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "✅ Modelo lag 21 guardado (fold 5) con RMSE 0.0495\n",
            "\n",
            "=== Entrenando modelo para lag 28 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "✅ Modelo lag 28 guardado (fold 5) con RMSE 0.0572\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Lags que quieres entrenar\n",
        "lags = [7, 14, 21, 28]\n",
        "\n",
        "for lag in lags:\n",
        "    print(f\"\\n=== Entrenando modelo para lag {lag} ===\")\n",
        "    data_dict = split_and_scale(timeSeries)\n",
        "    entrenar_y_guardar_modelo(data_dict, lag=lag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fec2cae",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b07795",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
