{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7ae2a77",
      "metadata": {
        "id": "d7ae2a77"
      },
      "source": [
        "# **Modelado: Volatilidad**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "11cf53c0",
      "metadata": {
        "id": "11cf53c0",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e3a535a4",
      "metadata": {
        "id": "e3a535a4",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "btc = pd.read_csv(r'https://raw.githubusercontent.com/TawnyVTC/Proyectos_UN/refs/heads/main/2025/Deep_Learning/Data/btc_1d_with_volatility_and_lags.csv')\n",
        "btc['Open time'] = pd.to_datetime(btc['Open time'], format='%Y-%m-%d')\n",
        "\n",
        "# -----------------------\n",
        "# 1. Crear lags de volatilidad\n",
        "# -----------------------\n",
        "for lag in [7, 14, 21, 28]:\n",
        "    btc[f'Volatility_lag_{lag}'] = btc['Volatility'].shift(lag)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Crear targets futuros (predicciones de volatilidad)\n",
        "# -----------------------\n",
        "for i in range(1, 8):  # t+1 a t+7\n",
        "    btc[f'target_vol_t+{i}'] = btc['Volatility'].shift(-i)\n",
        "\n",
        "# -----------------------\n",
        "# 3. Definir features y targets\n",
        "# -----------------------\n",
        "features = [\n",
        "    'Close', 'LogReturn', 'Volatility',\n",
        "    'Volatility_lag_7', 'Volatility_lag_14', 'Volatility_lag_21', 'Volatility_lag_28'\n",
        "]\n",
        "\n",
        "targets = [f'target_vol_t+{i}' for i in range(1, 8)]\n",
        "\n",
        "# -----------------------\n",
        "# 4. Eliminar filas con NaN (por los shifts)\n",
        "# -----------------------\n",
        "btc = btc.dropna(subset=features + targets).reset_index(drop=True)\n",
        "\n",
        "# -----------------------\n",
        "# 5. Definir matrices de entrada y salida\n",
        "# -----------------------\n",
        "X = btc[features].values\n",
        "y = btc[targets].values\n",
        "dates = btc[\"Open time\"].values\n",
        "\n",
        "# -----------------------\n",
        "# 6. ConstrucciÃ³n del objeto timeSeries\n",
        "# -----------------------\n",
        "timeSeries = np.concatenate([X, y], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "19dff0af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tsxv.splitTrainValTest import split_train_val_test_groupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def _ensure_2d(arr):\n",
        "    \"\"\"\n",
        "    Convierte el array a formato 2D.\n",
        "    Si llega 1D â†’ (n,1); si llega 3D â†’ (n, timesteps*feats)\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1:\n",
        "        return arr.reshape(-1, 1)\n",
        "    if arr.ndim == 2:\n",
        "        return arr\n",
        "    if arr.ndim == 3:\n",
        "        n, a, b = arr.shape\n",
        "        return arr.reshape(n, a * b)\n",
        "    raise ValueError(f\"Array con ndim={arr.ndim} no soportado por esta funciÃ³n.\")\n",
        "\n",
        "\n",
        "def split_and_scale(timeSeries, n_steps_input=7, n_steps_forecast=7, n_steps_jump=1, target_col=2):\n",
        "    X_list, y_list, Xcv_list, ycv_list, Xtest_list, ytest_list = split_train_val_test_groupKFold(\n",
        "        timeSeries,\n",
        "        n_steps_input,\n",
        "        n_steps_forecast,\n",
        "        n_steps_jump\n",
        "    )\n",
        "\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled = [], [], []\n",
        "    y_train_scaled, y_val_scaled, y_test_scaled = [], [], []\n",
        "    scalers_x, scalers_y = [], []\n",
        "\n",
        "    for fold in range(len(X_list)):\n",
        "        X_train_raw = _ensure_2d(X_list[fold])\n",
        "        X_val_raw   = _ensure_2d(Xcv_list[fold])\n",
        "        X_test_raw  = _ensure_2d(Xtest_list[fold])\n",
        "\n",
        "        # ğŸ‘‡ AquÃ­ filtramos solo la variable objetivo (columna target)\n",
        "        y_train_raw = _ensure_2d(y_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "        y_val_raw   = _ensure_2d(ycv_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "        y_test_raw  = _ensure_2d(ytest_list[fold])[:, target_col:target_col + n_steps_forecast]\n",
        "\n",
        "        # Escaladores\n",
        "        scaler_x = StandardScaler().fit(X_train_raw)\n",
        "        scaler_y = StandardScaler().fit(y_train_raw)\n",
        "\n",
        "        X_train_scaled.append(scaler_x.transform(X_train_raw))\n",
        "        X_val_scaled.append(scaler_x.transform(X_val_raw))\n",
        "        X_test_scaled.append(scaler_x.transform(X_test_raw))\n",
        "        y_train_scaled.append(scaler_y.transform(y_train_raw))\n",
        "        y_val_scaled.append(scaler_y.transform(y_val_raw))\n",
        "        y_test_scaled.append(scaler_y.transform(y_test_raw))\n",
        "\n",
        "        scalers_x.append(scaler_x)\n",
        "        scalers_y.append(scaler_y)\n",
        "\n",
        "        print(f\"Fold {fold+1}: train {X_train_raw.shape} -> val {X_val_raw.shape} -> test {X_test_raw.shape}\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_scaled,\n",
        "        'X_val': X_val_scaled,\n",
        "        'X_test': X_test_scaled,\n",
        "        'y_train': y_train_scaled,\n",
        "        'y_val': y_val_scaled,\n",
        "        'y_test': y_test_scaled,\n",
        "        'scalers_x': scalers_x,\n",
        "        'scalers_y': scalers_y\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b59078",
      "metadata": {},
      "source": [
        "from tsxv.splitTrainValTest import split_train_val_test_groupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def _ensure_2d(arr):\n",
        "    \"\"\"\n",
        "    Convierte el array a formato 2D.\n",
        "    Si llega 1D â†’ (n,1); si llega 3D â†’ (n, timesteps*feats)\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1:\n",
        "        return arr.reshape(-1, 1)\n",
        "    if arr.ndim == 2:\n",
        "        return arr\n",
        "    if arr.ndim == 3:\n",
        "        n, a, b = arr.shape\n",
        "        return arr.reshape(n, a * b)\n",
        "    raise ValueError(f\"Array con ndim={arr.ndim} no soportado por esta funciÃ³n.\")\n",
        "\n",
        "\n",
        "def preparar_volatilidad(btc, n_lag, n_outputs=7):\n",
        "    \"\"\"\n",
        "    Construye la matriz de caracterÃ­sticas y targets usando volatilidad como variable objetivo.\n",
        "    \"\"\"\n",
        "    # Variables base\n",
        "    features = ['Volatility', 'Close', 'LogReturn',\n",
        "                'Volatility_lag_7', 'Volatility_lag_14', 'Volatility_lag_21', 'Volatility_lag_28']\n",
        "    targets = [f'target_t+{i}' for i in range(1, n_outputs+1)]\n",
        "\n",
        "    X = btc[features].values\n",
        "    y = btc[targets].values\n",
        "    timeSeries = np.concatenate([X, y], axis=1)\n",
        "\n",
        "    X_list, y_list, Xcv_list, ycv_list, Xtest_list, ytest_list = split_train_val_test_groupKFold(\n",
        "        timeSeries, n_lag, n_outputs, 1\n",
        "    )\n",
        "\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled = [], [], []\n",
        "    y_train_scaled, y_val_scaled, y_test_scaled = [], [], []\n",
        "    scalers_x, scalers_y = [], []\n",
        "\n",
        "    for fold in range(len(X_list)):\n",
        "        X_train_raw = _ensure_2d(X_list[fold])\n",
        "        X_val_raw = _ensure_2d(Xcv_list[fold])\n",
        "        X_test_raw = _ensure_2d(Xtest_list[fold])\n",
        "\n",
        "        y_train_raw = _ensure_2d(y_list[fold])\n",
        "        y_val_raw = _ensure_2d(ycv_list[fold])\n",
        "        y_test_raw = _ensure_2d(ytest_list[fold])\n",
        "\n",
        "        scaler_x = StandardScaler().fit(X_train_raw)\n",
        "        scaler_y = StandardScaler().fit(y_train_raw)\n",
        "\n",
        "        X_train_scaled.append(scaler_x.transform(X_train_raw))\n",
        "        X_val_scaled.append(scaler_x.transform(X_val_raw))\n",
        "        X_test_scaled.append(scaler_x.transform(X_test_raw))\n",
        "        y_train_scaled.append(scaler_y.transform(y_train_raw))\n",
        "        y_val_scaled.append(scaler_y.transform(y_val_raw))\n",
        "        y_test_scaled.append(scaler_y.transform(y_test_raw))\n",
        "\n",
        "        scalers_x.append(scaler_x)\n",
        "        scalers_y.append(scaler_y)\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "        'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test_scaled,\n",
        "        'scalers_x': scalers_x, 'scalers_y': scalers_y\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "640ccfb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_independencia_residuos(residuos, lags=10):\n",
        "    resultado = acorr_ljungbox(residuos, lags=[lags], return_df=True)\n",
        "    return resultado['lb_pvalue'].iloc[0]\n",
        "\n",
        "def _calc_metrics_per_horizon(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true, y_pred: arrays (n_samples, n_horizons)\n",
        "    Devuelve DataFrame con mÃ©tricas por horizonte.\n",
        "    \"\"\"\n",
        "    n_outputs = y_true.shape[1]\n",
        "    rows = []\n",
        "    for h in range(n_outputs):\n",
        "        yt = y_true[:, h]\n",
        "        yp = y_pred[:, h]\n",
        "        mae = mean_absolute_error(yt, yp)\n",
        "        mse = mean_squared_error(yt, yp)\n",
        "        rmse = np.sqrt(mse)\n",
        "        # MAPE con epsilon para evitar divisiÃ³n por cero\n",
        "        mape = np.mean(np.abs((yt - yp) / (np.abs(yt) + 1e-8))) * 100\n",
        "        rows.append({'Horizonte': h+1, 'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape})\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4b3e7ab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def entrenar_y_guardar_modelo(data_dict, lag, n_outputs=7, epochs=100, batch_size=32, save_dir='models/volatilidad'):\n",
        "    resultados = []\n",
        "    for fold in range(len(data_dict['X_train'])):\n",
        "        X_train, X_val, X_test = data_dict['X_train'][fold], data_dict['X_val'][fold], data_dict['X_test'][fold]\n",
        "        y_train, y_val, y_test = data_dict['y_train'][fold], data_dict['y_val'][fold], data_dict['y_test'][fold]\n",
        "        scaler_y = data_dict['scalers_y'][fold]\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(n_outputs)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                  epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[es])\n",
        "\n",
        "        yhat_test = scaler_y.inverse_transform(model.predict(X_test))\n",
        "        ytest_real = scaler_y.inverse_transform(y_test)\n",
        "\n",
        "        df_metrics = _calc_metrics_per_horizon(ytest_real, yhat_test)\n",
        "        rmse_prom = df_metrics['RMSE'].mean()\n",
        "        resultados.append((fold, rmse_prom, model))\n",
        "\n",
        "    # Guardar el mejor modelo\n",
        "    best_fold, best_rmse, best_model = min(resultados, key=lambda x: x[1])\n",
        "    model_dir = os.path.join(save_dir, f'lag_{lag}')\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, f'mejor_fold_lag_{lag}.keras')\n",
        "    best_model.save(model_path)\n",
        "    print(f\"âœ… Modelo lag {lag} guardado (fold {best_fold+1}) con RMSE {best_rmse:.4f}\")\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "81d5ed4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Entrenando modelo para lag 7 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "âœ… Modelo lag 7 guardado (fold 4) con RMSE 0.0566\n",
            "\n",
            "=== Entrenando modelo para lag 14 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "âœ… Modelo lag 14 guardado (fold 2) con RMSE 0.0639\n",
            "\n",
            "=== Entrenando modelo para lag 21 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "âœ… Modelo lag 21 guardado (fold 5) con RMSE 0.0495\n",
            "\n",
            "=== Entrenando modelo para lag 28 ===\n",
            "Fold 1: train (325, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 2: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 3: train (323, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 4: train (322, 98) -> val (108, 98) -> test (108, 98)\n",
            "Fold 5: train (324, 98) -> val (108, 98) -> test (108, 98)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "âœ… Modelo lag 28 guardado (fold 5) con RMSE 0.0572\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Lags que quieres entrenar\n",
        "lags = [7, 14, 21, 28]\n",
        "\n",
        "for lag in lags:\n",
        "    print(f\"\\n=== Entrenando modelo para lag {lag} ===\")\n",
        "    data_dict = split_and_scale(timeSeries)\n",
        "    entrenar_y_guardar_modelo(data_dict, lag=lag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "6fec2cae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.119.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.49.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi)\n",
            "  Downloading pydantic-2.12.2-py3-none-any.whl.metadata (85 kB)\n",
            "     ---------------------------------------- 0.0/85.8 kB ? eta -:--:--\n",
            "     -------------------------------------- - 81.9/85.8 kB 2.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 85.8/85.8 kB 1.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.4 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
            "  Downloading pydantic_core-2.41.4-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\tawtoca\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.119.0-py3-none-any.whl (107 kB)\n",
            "   ---------------------------------------- 0.0/107.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 107.1/107.1 kB 6.5 MB/s eta 0:00:00\n",
            "Downloading uvicorn-0.37.0-py3-none-any.whl (67 kB)\n",
            "   ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 68.0/68.0 kB 3.8 MB/s eta 0:00:00\n",
            "Downloading pydantic-2.12.2-py3-none-any.whl (460 kB)\n",
            "   ---------------------------------------- 0.0/460.6 kB ? eta -:--:--\n",
            "   ------------------------- -------------- 297.0/460.6 kB 9.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 460.6/460.6 kB 7.1 MB/s eta 0:00:00\n",
            "Downloading pydantic_core-2.41.4-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.6/2.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.1/2.0 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.0/2.0 MB 14.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 11.7 MB/s eta 0:00:00\n",
            "Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "   ---------------------------------------- 0.0/73.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 73.7/73.7 kB 4.0 MB/s eta 0:00:00\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-extensions, annotated-types, uvicorn, typing-inspection, pydantic-core, starlette, pydantic, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "Successfully installed annotated-types-0.7.0 fastapi-0.119.0 pydantic-2.12.2 pydantic-core-2.41.4 starlette-0.48.0 typing-extensions-4.15.0 typing-inspection-0.4.2 uvicorn-0.37.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\TAWTOCA\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b07795",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
